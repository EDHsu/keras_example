{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple Neural Network with Keras\n",
    "\n",
    "1. Data Processing\n",
    "2. Define Model\n",
    "3. Compile Model\n",
    "4. Fit Model\n",
    "5. Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Anderson's Iris data set\n",
    "\n",
    "鳶尾花卉數據集（英文：Iris flower data set）或費雪鳶尾花卉數據集（英文：Fisher's Iris data set），是一類多重變量分析的數據集。它最初是埃德加·安德森從加拿大加斯帕半島上的鳶尾屬花朵中提取的地理變異數據[1]，後由羅納德·費雪作為判別分析的一個例子[2]，運用到統計學中。\n",
    "\n",
    "其數據集包含了150個樣本，都屬於鳶尾屬下的三個亞屬，分別是山鳶尾、變色鳶尾和維吉尼亞鳶尾。四個特徵被用作樣本的定量分析，它們分別是花萼和花瓣的長度和寬度。基於這四個特徵的集合，費雪發展了一個線性判別分析以確定其屬種。\n",
    "\n",
    "https://zh.wikipedia.org/wiki/安德森鸢尾花卉数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Attribute Information:\n",
    "\n",
    "1. sepal length in cm\n",
    "2. sepal width in cm\n",
    "3. petal length in cm\n",
    "4. petal width in cm\n",
    "5. class: Iris Setosa, Iris Versicolour, Iris Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1,3.5,1.4,0.2,Iris-setosa\r\n",
      "4.9,3.0,1.4,0.2,Iris-setosa\r\n",
      "4.7,3.2,1.3,0.2,Iris-setosa\r\n",
      "4.6,3.1,1.5,0.2,Iris-setosa\r\n",
      "5.0,3.6,1.4,0.2,Iris-setosa\r\n",
      "5.4,3.9,1.7,0.4,Iris-setosa\r\n",
      "4.6,3.4,1.4,0.3,Iris-setosa\r\n",
      "5.0,3.4,1.5,0.2,Iris-setosa\r\n",
      "4.4,2.9,1.4,0.2,Iris-setosa\r\n",
      "4.9,3.1,1.5,0.1,Iris-setosa\r\n",
      "5.4,3.7,1.5,0.2,Iris-setosa\r\n",
      "4.8,3.4,1.6,0.2,Iris-setosa\r\n",
      "4.8,3.0,1.4,0.1,Iris-setosa\r\n",
      "4.3,3.0,1.1,0.1,Iris-setosa\r\n",
      "5.8,4.0,1.2,0.2,Iris-setosa\r\n",
      "5.7,4.4,1.5,0.4,Iris-setosa\r\n",
      "5.4,3.9,1.3,0.4,Iris-setosa\r\n",
      "5.1,3.5,1.4,0.3,Iris-setosa\r\n",
      "5.7,3.8,1.7,0.3,Iris-setosa\r\n",
      "5.1,3.8,1.5,0.3,Iris-setosa\r\n",
      "5.4,3.4,1.7,0.2,Iris-setosa\r\n",
      "5.1,3.7,1.5,0.4,Iris-setosa\r\n",
      "4.6,3.6,1.0,0.2,Iris-setosa\r\n",
      "5.1,3.3,1.7,0.5,Iris-setosa\r\n",
      "4.8,3.4,1.9,0.2,Iris-setosa\r\n",
      "5.0,3.0,1.6,0.2,Iris-setosa\r\n",
      "5.0,3.4,1.6,0.4,Iris-setosa\r\n",
      "5.2,3.5,1.5,0.2,Iris-setosa\r\n",
      "5.2,3.4,1.4,0.2,Iris-setosa\r\n",
      "4.7,3.2,1.6,0.2,Iris-setosa\r\n",
      "4.8,3.1,1.6,0.2,Iris-setosa\r\n",
      "5.4,3.4,1.5,0.4,Iris-setosa\r\n",
      "5.2,4.1,1.5,0.1,Iris-setosa\r\n",
      "5.5,4.2,1.4,0.2,Iris-setosa\r\n",
      "4.9,3.1,1.5,0.1,Iris-setosa\r\n",
      "5.0,3.2,1.2,0.2,Iris-setosa\r\n",
      "5.5,3.5,1.3,0.2,Iris-setosa\r\n",
      "4.9,3.1,1.5,0.1,Iris-setosa\r\n",
      "4.4,3.0,1.3,0.2,Iris-setosa\r\n",
      "5.1,3.4,1.5,0.2,Iris-setosa\r\n",
      "5.0,3.5,1.3,0.3,Iris-setosa\r\n",
      "4.5,2.3,1.3,0.3,Iris-setosa\r\n",
      "4.4,3.2,1.3,0.2,Iris-setosa\r\n",
      "5.0,3.5,1.6,0.6,Iris-setosa\r\n",
      "5.1,3.8,1.9,0.4,Iris-setosa\r\n",
      "4.8,3.0,1.4,0.3,Iris-setosa\r\n",
      "5.1,3.8,1.6,0.2,Iris-setosa\r\n",
      "4.6,3.2,1.4,0.2,Iris-setosa\r\n",
      "5.3,3.7,1.5,0.2,Iris-setosa\r\n",
      "5.0,3.3,1.4,0.2,Iris-setosa\r\n",
      "7.0,3.2,4.7,1.4,Iris-versicolor\r\n",
      "6.4,3.2,4.5,1.5,Iris-versicolor\r\n",
      "6.9,3.1,4.9,1.5,Iris-versicolor\r\n",
      "5.5,2.3,4.0,1.3,Iris-versicolor\r\n",
      "6.5,2.8,4.6,1.5,Iris-versicolor\r\n",
      "5.7,2.8,4.5,1.3,Iris-versicolor\r\n",
      "6.3,3.3,4.7,1.6,Iris-versicolor\r\n",
      "4.9,2.4,3.3,1.0,Iris-versicolor\r\n",
      "6.6,2.9,4.6,1.3,Iris-versicolor\r\n",
      "5.2,2.7,3.9,1.4,Iris-versicolor\r\n",
      "5.0,2.0,3.5,1.0,Iris-versicolor\r\n",
      "5.9,3.0,4.2,1.5,Iris-versicolor\r\n",
      "6.0,2.2,4.0,1.0,Iris-versicolor\r\n",
      "6.1,2.9,4.7,1.4,Iris-versicolor\r\n",
      "5.6,2.9,3.6,1.3,Iris-versicolor\r\n",
      "6.7,3.1,4.4,1.4,Iris-versicolor\r\n",
      "5.6,3.0,4.5,1.5,Iris-versicolor\r\n",
      "5.8,2.7,4.1,1.0,Iris-versicolor\r\n",
      "6.2,2.2,4.5,1.5,Iris-versicolor\r\n",
      "5.6,2.5,3.9,1.1,Iris-versicolor\r\n",
      "5.9,3.2,4.8,1.8,Iris-versicolor\r\n",
      "6.1,2.8,4.0,1.3,Iris-versicolor\r\n",
      "6.3,2.5,4.9,1.5,Iris-versicolor\r\n",
      "6.1,2.8,4.7,1.2,Iris-versicolor\r\n",
      "6.4,2.9,4.3,1.3,Iris-versicolor\r\n",
      "6.6,3.0,4.4,1.4,Iris-versicolor\r\n",
      "6.8,2.8,4.8,1.4,Iris-versicolor\r\n",
      "6.7,3.0,5.0,1.7,Iris-versicolor\r\n",
      "6.0,2.9,4.5,1.5,Iris-versicolor\r\n",
      "5.7,2.6,3.5,1.0,Iris-versicolor\r\n",
      "5.5,2.4,3.8,1.1,Iris-versicolor\r\n",
      "5.5,2.4,3.7,1.0,Iris-versicolor\r\n",
      "5.8,2.7,3.9,1.2,Iris-versicolor\r\n",
      "6.0,2.7,5.1,1.6,Iris-versicolor\r\n",
      "5.4,3.0,4.5,1.5,Iris-versicolor\r\n",
      "6.0,3.4,4.5,1.6,Iris-versicolor\r\n",
      "6.7,3.1,4.7,1.5,Iris-versicolor\r\n",
      "6.3,2.3,4.4,1.3,Iris-versicolor\r\n",
      "5.6,3.0,4.1,1.3,Iris-versicolor\r\n",
      "5.5,2.5,4.0,1.3,Iris-versicolor\r\n",
      "5.5,2.6,4.4,1.2,Iris-versicolor\r\n",
      "6.1,3.0,4.6,1.4,Iris-versicolor\r\n",
      "5.8,2.6,4.0,1.2,Iris-versicolor\r\n",
      "5.0,2.3,3.3,1.0,Iris-versicolor\r\n",
      "5.6,2.7,4.2,1.3,Iris-versicolor\r\n",
      "5.7,3.0,4.2,1.2,Iris-versicolor\r\n",
      "5.7,2.9,4.2,1.3,Iris-versicolor\r\n",
      "6.2,2.9,4.3,1.3,Iris-versicolor\r\n",
      "5.1,2.5,3.0,1.1,Iris-versicolor\r\n",
      "5.7,2.8,4.1,1.3,Iris-versicolor\r\n",
      "6.3,3.3,6.0,2.5,Iris-virginica\r\n",
      "5.8,2.7,5.1,1.9,Iris-virginica\r\n",
      "7.1,3.0,5.9,2.1,Iris-virginica\r\n",
      "6.3,2.9,5.6,1.8,Iris-virginica\r\n",
      "6.5,3.0,5.8,2.2,Iris-virginica\r\n",
      "7.6,3.0,6.6,2.1,Iris-virginica\r\n",
      "4.9,2.5,4.5,1.7,Iris-virginica\r\n",
      "7.3,2.9,6.3,1.8,Iris-virginica\r\n",
      "6.7,2.5,5.8,1.8,Iris-virginica\r\n",
      "7.2,3.6,6.1,2.5,Iris-virginica\r\n",
      "6.5,3.2,5.1,2.0,Iris-virginica\r\n",
      "6.4,2.7,5.3,1.9,Iris-virginica\r\n",
      "6.8,3.0,5.5,2.1,Iris-virginica\r\n",
      "5.7,2.5,5.0,2.0,Iris-virginica\r\n",
      "5.8,2.8,5.1,2.4,Iris-virginica\r\n",
      "6.4,3.2,5.3,2.3,Iris-virginica\r\n",
      "6.5,3.0,5.5,1.8,Iris-virginica\r\n",
      "7.7,3.8,6.7,2.2,Iris-virginica\r\n",
      "7.7,2.6,6.9,2.3,Iris-virginica\r\n",
      "6.0,2.2,5.0,1.5,Iris-virginica\r\n",
      "6.9,3.2,5.7,2.3,Iris-virginica\r\n",
      "5.6,2.8,4.9,2.0,Iris-virginica\r\n",
      "7.7,2.8,6.7,2.0,Iris-virginica\r\n",
      "6.3,2.7,4.9,1.8,Iris-virginica\r\n",
      "6.7,3.3,5.7,2.1,Iris-virginica\r\n",
      "7.2,3.2,6.0,1.8,Iris-virginica\r\n",
      "6.2,2.8,4.8,1.8,Iris-virginica\r\n",
      "6.1,3.0,4.9,1.8,Iris-virginica\r\n",
      "6.4,2.8,5.6,2.1,Iris-virginica\r\n",
      "7.2,3.0,5.8,1.6,Iris-virginica\r\n",
      "7.4,2.8,6.1,1.9,Iris-virginica\r\n",
      "7.9,3.8,6.4,2.0,Iris-virginica\r\n",
      "6.4,2.8,5.6,2.2,Iris-virginica\r\n",
      "6.3,2.8,5.1,1.5,Iris-virginica\r\n",
      "6.1,2.6,5.6,1.4,Iris-virginica\r\n",
      "7.7,3.0,6.1,2.3,Iris-virginica\r\n",
      "6.3,3.4,5.6,2.4,Iris-virginica\r\n",
      "6.4,3.1,5.5,1.8,Iris-virginica\r\n",
      "6.0,3.0,4.8,1.8,Iris-virginica\r\n",
      "6.9,3.1,5.4,2.1,Iris-virginica\r\n",
      "6.7,3.1,5.6,2.4,Iris-virginica\r\n",
      "6.9,3.1,5.1,2.3,Iris-virginica\r\n",
      "5.8,2.7,5.1,1.9,Iris-virginica\r\n",
      "6.8,3.2,5.9,2.3,Iris-virginica\r\n",
      "6.7,3.3,5.7,2.5,Iris-virginica\r\n",
      "6.7,3.0,5.2,2.3,Iris-virginica\r\n",
      "6.3,2.5,5.0,1.9,Iris-virginica\r\n",
      "6.5,3.0,5.2,2.0,Iris-virginica\r\n",
      "6.2,3.4,5.4,2.3,Iris-virginica\r\n",
      "5.9,3.0,5.1,1.8,Iris-virginica\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "cat iris.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import package\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# setup random seed for reproducibility\n",
    "seed = 10\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data_frame = pandas.read_csv(\"iris.data\", header=None)\n",
    "data_set = data_frame.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3            4\n",
       "0  5.1  3.5  1.4  0.2  Iris-setosa\n",
       "1  4.9  3.0  1.4  0.2  Iris-setosa\n",
       "2  4.7  3.2  1.3  0.2  Iris-setosa\n",
       "3  4.6  3.1  1.5  0.2  Iris-setosa\n",
       "4  5.0  3.6  1.4  0.2  Iris-setosa"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "X = data_set[:,0:4].astype(float)\n",
    "Y = data_set[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# one hot encoded, convert integers to dummy variables\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Define Model\n",
    "\n",
    "Models in Keras are defined as a sequence of layers. create a Sequential model and add layers. The first thing to get right is to ensure the input layer has the right number of inputs.\n",
    "\n",
    "Input Layer (4 input) -> [2 Hidden Layer (4 neurons and 12 neurons)] -> Outpout Layer (3 output)\n",
    "\n",
    "In this example. we use a fully-connected network structure. Fully connected layers are defined using the \"Dense\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(4, input_dim=4, init='normal', activation='relu'))\n",
    "model.add(Dense(12, init='normal', activation='relu'))\n",
    "model.add(Dense(3, init='normal', activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Compile Model\n",
    "\n",
    "Now that the model is defined, we can compile it with backend, such as Theano or TensorFlow. In this example, we use the efficient gradient descent algorithm adam here. You can also setup parameter by yourself. Remember training a network means finding the best set of weights to make predictions for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# compile model with different optimizer, like \"sgd\" or \"adam\"\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# or you can define optimizer by yourself\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fit Model\n",
    "\n",
    "We can train or fit our model on our loaded data by calling the fit() function on the model. For this problem we will run for a small number of epochs (150) and use a relatively small batch size of 10. Again, these can be chosen experimentally by trial and error.\n",
    "\n",
    "Keras can separate a portion of your training data into a validation dataset and evaluate the performance of your model on that validation dataset each epoch. You can do this by setting the validation split argument on the fit() function to a percentage of the size of your training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/150\n",
      "120/120 [==============================] - 0s - loss: 1.0975 - acc: 0.4167 - val_loss: 1.1088 - val_acc: 0.0000e+00\n",
      "Epoch 2/150\n",
      "120/120 [==============================] - 0s - loss: 1.0933 - acc: 0.4167 - val_loss: 1.1282 - val_acc: 0.0000e+00\n",
      "Epoch 3/150\n",
      "120/120 [==============================] - 0s - loss: 1.0877 - acc: 0.4167 - val_loss: 1.1473 - val_acc: 0.0000e+00\n",
      "Epoch 4/150\n",
      "120/120 [==============================] - 0s - loss: 1.0813 - acc: 0.4167 - val_loss: 1.1703 - val_acc: 0.0000e+00\n",
      "Epoch 5/150\n",
      "120/120 [==============================] - 0s - loss: 1.0748 - acc: 0.4167 - val_loss: 1.2069 - val_acc: 0.0000e+00\n",
      "Epoch 6/150\n",
      "120/120 [==============================] - 0s - loss: 1.0637 - acc: 0.4167 - val_loss: 1.2333 - val_acc: 0.0000e+00\n",
      "Epoch 7/150\n",
      "120/120 [==============================] - 0s - loss: 1.0525 - acc: 0.4167 - val_loss: 1.2655 - val_acc: 0.0000e+00\n",
      "Epoch 8/150\n",
      "120/120 [==============================] - 0s - loss: 1.0377 - acc: 0.4167 - val_loss: 1.2903 - val_acc: 0.0000e+00\n",
      "Epoch 9/150\n",
      "120/120 [==============================] - 0s - loss: 1.0219 - acc: 0.4167 - val_loss: 1.3370 - val_acc: 0.0000e+00\n",
      "Epoch 10/150\n",
      "120/120 [==============================] - 0s - loss: 1.0004 - acc: 0.4333 - val_loss: 1.3585 - val_acc: 0.0000e+00\n",
      "Epoch 11/150\n",
      "120/120 [==============================] - 0s - loss: 0.9773 - acc: 0.5000 - val_loss: 1.3797 - val_acc: 0.0000e+00\n",
      "Epoch 12/150\n",
      "120/120 [==============================] - 0s - loss: 0.9508 - acc: 0.7000 - val_loss: 1.3988 - val_acc: 0.0000e+00\n",
      "Epoch 13/150\n",
      "120/120 [==============================] - 0s - loss: 0.9209 - acc: 0.8167 - val_loss: 1.4147 - val_acc: 0.0000e+00\n",
      "Epoch 14/150\n",
      "120/120 [==============================] - 0s - loss: 0.8853 - acc: 0.8333 - val_loss: 1.3983 - val_acc: 0.0000e+00\n",
      "Epoch 15/150\n",
      "120/120 [==============================] - 0s - loss: 0.8465 - acc: 0.8333 - val_loss: 1.3974 - val_acc: 0.0000e+00\n",
      "Epoch 16/150\n",
      "120/120 [==============================] - 0s - loss: 0.8038 - acc: 0.8333 - val_loss: 1.3595 - val_acc: 0.0000e+00\n",
      "Epoch 17/150\n",
      "120/120 [==============================] - 0s - loss: 0.7593 - acc: 0.8333 - val_loss: 1.3303 - val_acc: 0.0000e+00\n",
      "Epoch 18/150\n",
      "120/120 [==============================] - 0s - loss: 0.7134 - acc: 0.8333 - val_loss: 1.3056 - val_acc: 0.0000e+00\n",
      "Epoch 19/150\n",
      "120/120 [==============================] - 0s - loss: 0.6695 - acc: 0.8333 - val_loss: 1.2944 - val_acc: 0.0000e+00\n",
      "Epoch 20/150\n",
      "120/120 [==============================] - 0s - loss: 0.6252 - acc: 0.8333 - val_loss: 1.2811 - val_acc: 0.0000e+00\n",
      "Epoch 21/150\n",
      "120/120 [==============================] - 0s - loss: 0.5849 - acc: 0.8333 - val_loss: 1.2392 - val_acc: 0.0000e+00\n",
      "Epoch 22/150\n",
      "120/120 [==============================] - 0s - loss: 0.5486 - acc: 0.8333 - val_loss: 1.2413 - val_acc: 0.0000e+00\n",
      "Epoch 23/150\n",
      "120/120 [==============================] - 0s - loss: 0.5148 - acc: 0.8333 - val_loss: 1.1871 - val_acc: 0.0000e+00\n",
      "Epoch 24/150\n",
      "120/120 [==============================] - 0s - loss: 0.4866 - acc: 0.8333 - val_loss: 1.1347 - val_acc: 0.0000e+00\n",
      "Epoch 25/150\n",
      "120/120 [==============================] - 0s - loss: 0.4606 - acc: 0.8333 - val_loss: 1.1062 - val_acc: 0.0000e+00\n",
      "Epoch 26/150\n",
      "120/120 [==============================] - 0s - loss: 0.4407 - acc: 0.8333 - val_loss: 1.0658 - val_acc: 0.0000e+00\n",
      "Epoch 27/150\n",
      "120/120 [==============================] - 0s - loss: 0.4205 - acc: 0.8333 - val_loss: 1.0592 - val_acc: 0.0000e+00\n",
      "Epoch 28/150\n",
      "120/120 [==============================] - 0s - loss: 0.4073 - acc: 0.8333 - val_loss: 1.1130 - val_acc: 0.0000e+00\n",
      "Epoch 29/150\n",
      "120/120 [==============================] - 0s - loss: 0.3910 - acc: 0.8333 - val_loss: 1.0191 - val_acc: 0.0000e+00\n",
      "Epoch 30/150\n",
      "120/120 [==============================] - 0s - loss: 0.3782 - acc: 0.8333 - val_loss: 1.0424 - val_acc: 0.0000e+00\n",
      "Epoch 31/150\n",
      "120/120 [==============================] - 0s - loss: 0.3654 - acc: 0.8333 - val_loss: 0.9834 - val_acc: 0.0000e+00\n",
      "Epoch 32/150\n",
      "120/120 [==============================] - 0s - loss: 0.3558 - acc: 0.8333 - val_loss: 0.9428 - val_acc: 0.0000e+00\n",
      "Epoch 33/150\n",
      "120/120 [==============================] - 0s - loss: 0.3464 - acc: 0.8333 - val_loss: 0.9850 - val_acc: 0.0000e+00\n",
      "Epoch 34/150\n",
      "120/120 [==============================] - 0s - loss: 0.3362 - acc: 0.8333 - val_loss: 0.9439 - val_acc: 0.0000e+00\n",
      "Epoch 35/150\n",
      "120/120 [==============================] - 0s - loss: 0.3268 - acc: 0.8333 - val_loss: 0.9411 - val_acc: 0.0000e+00\n",
      "Epoch 36/150\n",
      "120/120 [==============================] - 0s - loss: 0.3194 - acc: 0.8417 - val_loss: 0.8699 - val_acc: 0.0000e+00\n",
      "Epoch 37/150\n",
      "120/120 [==============================] - 0s - loss: 0.3103 - acc: 0.8417 - val_loss: 0.9149 - val_acc: 0.0000e+00\n",
      "Epoch 38/150\n",
      "120/120 [==============================] - 0s - loss: 0.3024 - acc: 0.8500 - val_loss: 0.9050 - val_acc: 0.0000e+00\n",
      "Epoch 39/150\n",
      "120/120 [==============================] - 0s - loss: 0.2945 - acc: 0.8500 - val_loss: 0.8414 - val_acc: 0.0667\n",
      "Epoch 40/150\n",
      "120/120 [==============================] - 0s - loss: 0.2860 - acc: 0.8917 - val_loss: 0.8422 - val_acc: 0.1000\n",
      "Epoch 41/150\n",
      "120/120 [==============================] - 0s - loss: 0.2784 - acc: 0.8667 - val_loss: 0.8798 - val_acc: 0.0667\n",
      "Epoch 42/150\n",
      "120/120 [==============================] - 0s - loss: 0.2729 - acc: 0.8833 - val_loss: 0.8361 - val_acc: 0.2333\n",
      "Epoch 43/150\n",
      "120/120 [==============================] - 0s - loss: 0.2640 - acc: 0.8917 - val_loss: 0.8060 - val_acc: 0.2667\n",
      "Epoch 44/150\n",
      "120/120 [==============================] - 0s - loss: 0.2577 - acc: 0.8917 - val_loss: 0.8396 - val_acc: 0.2333\n",
      "Epoch 45/150\n",
      "120/120 [==============================] - 0s - loss: 0.2517 - acc: 0.9250 - val_loss: 0.7562 - val_acc: 0.3667\n",
      "Epoch 46/150\n",
      "120/120 [==============================] - 0s - loss: 0.2448 - acc: 0.9000 - val_loss: 0.8158 - val_acc: 0.2667\n",
      "Epoch 47/150\n",
      "120/120 [==============================] - 0s - loss: 0.2382 - acc: 0.9333 - val_loss: 0.7044 - val_acc: 0.5000\n",
      "Epoch 48/150\n",
      "120/120 [==============================] - 0s - loss: 0.2313 - acc: 0.9417 - val_loss: 0.7315 - val_acc: 0.4667\n",
      "Epoch 49/150\n",
      "120/120 [==============================] - 0s - loss: 0.2258 - acc: 0.9667 - val_loss: 0.6903 - val_acc: 0.5333\n",
      "Epoch 50/150\n",
      "120/120 [==============================] - 0s - loss: 0.2210 - acc: 0.9417 - val_loss: 0.7643 - val_acc: 0.3667\n",
      "Epoch 51/150\n",
      "120/120 [==============================] - 0s - loss: 0.2138 - acc: 0.9667 - val_loss: 0.6768 - val_acc: 0.5333\n",
      "Epoch 52/150\n",
      "120/120 [==============================] - 0s - loss: 0.2077 - acc: 0.9750 - val_loss: 0.6730 - val_acc: 0.5333\n",
      "Epoch 53/150\n",
      "120/120 [==============================] - 0s - loss: 0.2059 - acc: 0.9500 - val_loss: 0.7537 - val_acc: 0.4667\n",
      "Epoch 54/150\n",
      "120/120 [==============================] - 0s - loss: 0.1989 - acc: 0.9667 - val_loss: 0.6285 - val_acc: 0.6333\n",
      "Epoch 55/150\n",
      "120/120 [==============================] - 0s - loss: 0.1927 - acc: 0.9833 - val_loss: 0.6849 - val_acc: 0.5333\n",
      "Epoch 56/150\n",
      "120/120 [==============================] - 0s - loss: 0.1878 - acc: 0.9750 - val_loss: 0.6936 - val_acc: 0.5333\n",
      "Epoch 57/150\n",
      "120/120 [==============================] - 0s - loss: 0.1814 - acc: 0.9833 - val_loss: 0.5750 - val_acc: 0.7000\n",
      "Epoch 58/150\n",
      "120/120 [==============================] - 0s - loss: 0.1787 - acc: 0.9917 - val_loss: 0.6304 - val_acc: 0.6000\n",
      "Epoch 59/150\n",
      "120/120 [==============================] - 0s - loss: 0.1753 - acc: 0.9667 - val_loss: 0.6782 - val_acc: 0.5333\n",
      "Epoch 60/150\n",
      "120/120 [==============================] - 0s - loss: 0.1674 - acc: 0.9833 - val_loss: 0.5476 - val_acc: 0.7333\n",
      "Epoch 61/150\n",
      "120/120 [==============================] - 0s - loss: 0.1665 - acc: 0.9833 - val_loss: 0.5278 - val_acc: 0.7667\n",
      "Epoch 62/150\n",
      "120/120 [==============================] - 0s - loss: 0.1627 - acc: 0.9750 - val_loss: 0.7060 - val_acc: 0.5333\n",
      "Epoch 63/150\n",
      "120/120 [==============================] - 0s - loss: 0.1578 - acc: 0.9833 - val_loss: 0.5531 - val_acc: 0.7333\n",
      "Epoch 64/150\n",
      "120/120 [==============================] - 0s - loss: 0.1524 - acc: 0.9917 - val_loss: 0.5311 - val_acc: 0.7333\n",
      "Epoch 65/150\n",
      "120/120 [==============================] - 0s - loss: 0.1503 - acc: 0.9833 - val_loss: 0.5847 - val_acc: 0.6667\n",
      "Epoch 66/150\n",
      "120/120 [==============================] - 0s - loss: 0.1469 - acc: 0.9833 - val_loss: 0.4880 - val_acc: 0.7667\n",
      "Epoch 67/150\n",
      "120/120 [==============================] - 0s - loss: 0.1431 - acc: 0.9917 - val_loss: 0.5787 - val_acc: 0.6667\n",
      "Epoch 68/150\n",
      "120/120 [==============================] - 0s - loss: 0.1389 - acc: 0.9917 - val_loss: 0.5151 - val_acc: 0.7333\n",
      "Epoch 69/150\n",
      "120/120 [==============================] - 0s - loss: 0.1359 - acc: 0.9917 - val_loss: 0.5144 - val_acc: 0.7333\n",
      "Epoch 70/150\n",
      "120/120 [==============================] - 0s - loss: 0.1320 - acc: 0.9917 - val_loss: 0.5429 - val_acc: 0.7000\n",
      "Epoch 71/150\n",
      "120/120 [==============================] - 0s - loss: 0.1294 - acc: 0.9833 - val_loss: 0.4776 - val_acc: 0.7667\n",
      "Epoch 72/150\n",
      "120/120 [==============================] - 0s - loss: 0.1266 - acc: 0.9917 - val_loss: 0.5006 - val_acc: 0.7667\n",
      "Epoch 73/150\n",
      "120/120 [==============================] - 0s - loss: 0.1231 - acc: 0.9917 - val_loss: 0.5092 - val_acc: 0.7333\n",
      "Epoch 74/150\n",
      "120/120 [==============================] - 0s - loss: 0.1212 - acc: 0.9917 - val_loss: 0.5372 - val_acc: 0.7000\n",
      "Epoch 75/150\n",
      "120/120 [==============================] - 0s - loss: 0.1219 - acc: 0.9833 - val_loss: 0.4164 - val_acc: 0.8000\n",
      "Epoch 76/150\n",
      "120/120 [==============================] - 0s - loss: 0.1170 - acc: 0.9833 - val_loss: 0.5419 - val_acc: 0.6667\n",
      "Epoch 77/150\n",
      "120/120 [==============================] - 0s - loss: 0.1147 - acc: 0.9917 - val_loss: 0.5099 - val_acc: 0.7333\n",
      "Epoch 78/150\n",
      "120/120 [==============================] - 0s - loss: 0.1101 - acc: 0.9833 - val_loss: 0.4356 - val_acc: 0.8000\n",
      "Epoch 79/150\n",
      "120/120 [==============================] - 0s - loss: 0.1088 - acc: 0.9833 - val_loss: 0.4548 - val_acc: 0.7667\n",
      "Epoch 80/150\n",
      "120/120 [==============================] - 0s - loss: 0.1066 - acc: 0.9833 - val_loss: 0.4439 - val_acc: 0.8000\n",
      "Epoch 81/150\n",
      "120/120 [==============================] - 0s - loss: 0.1052 - acc: 0.9917 - val_loss: 0.4549 - val_acc: 0.7667\n",
      "Epoch 82/150\n",
      "120/120 [==============================] - 0s - loss: 0.1026 - acc: 0.9833 - val_loss: 0.4230 - val_acc: 0.8000\n",
      "Epoch 83/150\n",
      "120/120 [==============================] - 0s - loss: 0.1005 - acc: 0.9833 - val_loss: 0.4226 - val_acc: 0.8000\n",
      "Epoch 84/150\n",
      "120/120 [==============================] - 0s - loss: 0.0984 - acc: 0.9917 - val_loss: 0.4645 - val_acc: 0.7667\n",
      "Epoch 85/150\n",
      "120/120 [==============================] - 0s - loss: 0.0982 - acc: 0.9833 - val_loss: 0.4320 - val_acc: 0.8000\n",
      "Epoch 86/150\n",
      "120/120 [==============================] - 0s - loss: 0.0964 - acc: 0.9833 - val_loss: 0.3982 - val_acc: 0.8000\n",
      "Epoch 87/150\n",
      "120/120 [==============================] - 0s - loss: 0.0951 - acc: 0.9833 - val_loss: 0.5128 - val_acc: 0.7333\n",
      "Epoch 88/150\n",
      "120/120 [==============================] - 0s - loss: 0.0913 - acc: 0.9917 - val_loss: 0.4190 - val_acc: 0.8000\n",
      "Epoch 89/150\n",
      "120/120 [==============================] - 0s - loss: 0.0914 - acc: 0.9917 - val_loss: 0.3342 - val_acc: 0.8333\n",
      "Epoch 90/150\n",
      "120/120 [==============================] - 0s - loss: 0.0889 - acc: 0.9833 - val_loss: 0.4519 - val_acc: 0.7667\n",
      "Epoch 91/150\n",
      "120/120 [==============================] - 0s - loss: 0.0876 - acc: 0.9833 - val_loss: 0.4129 - val_acc: 0.8000\n",
      "Epoch 92/150\n",
      "120/120 [==============================] - 0s - loss: 0.0887 - acc: 0.9833 - val_loss: 0.3822 - val_acc: 0.8000\n",
      "Epoch 93/150\n",
      "120/120 [==============================] - 0s - loss: 0.0861 - acc: 0.9833 - val_loss: 0.5208 - val_acc: 0.6667\n",
      "Epoch 94/150\n",
      "120/120 [==============================] - 0s - loss: 0.0894 - acc: 0.9917 - val_loss: 0.3353 - val_acc: 0.8333\n",
      "Epoch 95/150\n",
      "120/120 [==============================] - 0s - loss: 0.0819 - acc: 0.9833 - val_loss: 0.4465 - val_acc: 0.7667\n",
      "Epoch 96/150\n",
      "120/120 [==============================] - 0s - loss: 0.0813 - acc: 0.9917 - val_loss: 0.3802 - val_acc: 0.8000\n",
      "Epoch 97/150\n",
      "120/120 [==============================] - 0s - loss: 0.0799 - acc: 0.9833 - val_loss: 0.4478 - val_acc: 0.7667\n",
      "Epoch 98/150\n",
      "120/120 [==============================] - 0s - loss: 0.0783 - acc: 0.9917 - val_loss: 0.3558 - val_acc: 0.8000\n",
      "Epoch 99/150\n",
      "120/120 [==============================] - 0s - loss: 0.0774 - acc: 0.9833 - val_loss: 0.3921 - val_acc: 0.8000\n",
      "Epoch 100/150\n",
      "120/120 [==============================] - 0s - loss: 0.0787 - acc: 0.9917 - val_loss: 0.3902 - val_acc: 0.8000\n",
      "Epoch 101/150\n",
      "120/120 [==============================] - 0s - loss: 0.0761 - acc: 0.9917 - val_loss: 0.3006 - val_acc: 0.8667\n",
      "Epoch 102/150\n",
      "120/120 [==============================] - 0s - loss: 0.0731 - acc: 0.9833 - val_loss: 0.4115 - val_acc: 0.8000\n",
      "Epoch 103/150\n",
      "120/120 [==============================] - 0s - loss: 0.0773 - acc: 0.9833 - val_loss: 0.4491 - val_acc: 0.7667\n",
      "Epoch 104/150\n",
      "120/120 [==============================] - 0s - loss: 0.0800 - acc: 0.9917 - val_loss: 0.2823 - val_acc: 0.9000\n",
      "Epoch 105/150\n",
      "120/120 [==============================] - 0s - loss: 0.0740 - acc: 0.9917 - val_loss: 0.4686 - val_acc: 0.7667\n",
      "Epoch 106/150\n",
      "120/120 [==============================] - 0s - loss: 0.0698 - acc: 0.9833 - val_loss: 0.3663 - val_acc: 0.8000\n",
      "Epoch 107/150\n",
      "120/120 [==============================] - 0s - loss: 0.0688 - acc: 0.9917 - val_loss: 0.3375 - val_acc: 0.8000\n",
      "Epoch 108/150\n",
      "120/120 [==============================] - 0s - loss: 0.0682 - acc: 0.9833 - val_loss: 0.3470 - val_acc: 0.8000\n",
      "Epoch 109/150\n",
      "120/120 [==============================] - 0s - loss: 0.0681 - acc: 0.9833 - val_loss: 0.4583 - val_acc: 0.7667\n",
      "Epoch 110/150\n",
      "120/120 [==============================] - 0s - loss: 0.0680 - acc: 0.9833 - val_loss: 0.3437 - val_acc: 0.8000\n",
      "Epoch 111/150\n",
      "120/120 [==============================] - 0s - loss: 0.0655 - acc: 0.9833 - val_loss: 0.3719 - val_acc: 0.8000\n",
      "Epoch 112/150\n",
      "120/120 [==============================] - 0s - loss: 0.0660 - acc: 0.9833 - val_loss: 0.3989 - val_acc: 0.8000\n",
      "Epoch 113/150\n",
      "120/120 [==============================] - 0s - loss: 0.0662 - acc: 0.9917 - val_loss: 0.3035 - val_acc: 0.8333\n",
      "Epoch 114/150\n",
      "120/120 [==============================] - 0s - loss: 0.0626 - acc: 0.9917 - val_loss: 0.3596 - val_acc: 0.8000\n",
      "Epoch 115/150\n",
      "120/120 [==============================] - 0s - loss: 0.0641 - acc: 0.9833 - val_loss: 0.3893 - val_acc: 0.8000\n",
      "Epoch 116/150\n",
      "120/120 [==============================] - 0s - loss: 0.0633 - acc: 0.9917 - val_loss: 0.4260 - val_acc: 0.7667\n",
      "Epoch 117/150\n",
      "120/120 [==============================] - 0s - loss: 0.0629 - acc: 0.9833 - val_loss: 0.3175 - val_acc: 0.8000\n",
      "Epoch 118/150\n",
      "120/120 [==============================] - 0s - loss: 0.0604 - acc: 0.9833 - val_loss: 0.3525 - val_acc: 0.8000\n",
      "Epoch 119/150\n",
      "120/120 [==============================] - 0s - loss: 0.0640 - acc: 0.9833 - val_loss: 0.4402 - val_acc: 0.7667\n",
      "Epoch 120/150\n",
      "120/120 [==============================] - 0s - loss: 0.0624 - acc: 0.9917 - val_loss: 0.2758 - val_acc: 0.8667\n",
      "Epoch 121/150\n",
      "120/120 [==============================] - 0s - loss: 0.0584 - acc: 0.9917 - val_loss: 0.3570 - val_acc: 0.8000\n",
      "Epoch 122/150\n",
      "120/120 [==============================] - 0s - loss: 0.0593 - acc: 0.9833 - val_loss: 0.3592 - val_acc: 0.8000\n",
      "Epoch 123/150\n",
      "120/120 [==============================] - 0s - loss: 0.0582 - acc: 0.9833 - val_loss: 0.3165 - val_acc: 0.8000\n",
      "Epoch 124/150\n",
      "120/120 [==============================] - 0s - loss: 0.0572 - acc: 0.9833 - val_loss: 0.3744 - val_acc: 0.8000\n",
      "Epoch 125/150\n",
      "120/120 [==============================] - 0s - loss: 0.0571 - acc: 0.9833 - val_loss: 0.3945 - val_acc: 0.8000\n",
      "Epoch 126/150\n",
      "120/120 [==============================] - 0s - loss: 0.0567 - acc: 0.9833 - val_loss: 0.3049 - val_acc: 0.8333\n",
      "Epoch 127/150\n",
      "120/120 [==============================] - 0s - loss: 0.0551 - acc: 0.9917 - val_loss: 0.3669 - val_acc: 0.8000\n",
      "Epoch 128/150\n",
      "120/120 [==============================] - 0s - loss: 0.0555 - acc: 0.9833 - val_loss: 0.3756 - val_acc: 0.8000\n",
      "Epoch 129/150\n",
      "120/120 [==============================] - 0s - loss: 0.0541 - acc: 0.9833 - val_loss: 0.3210 - val_acc: 0.8000\n",
      "Epoch 130/150\n",
      "120/120 [==============================] - 0s - loss: 0.0564 - acc: 0.9833 - val_loss: 0.3780 - val_acc: 0.8000\n",
      "Epoch 131/150\n",
      "120/120 [==============================] - 0s - loss: 0.0552 - acc: 0.9833 - val_loss: 0.2820 - val_acc: 0.8333\n",
      "Epoch 132/150\n",
      "120/120 [==============================] - 0s - loss: 0.0544 - acc: 0.9917 - val_loss: 0.3427 - val_acc: 0.8000\n",
      "Epoch 133/150\n",
      "120/120 [==============================] - 0s - loss: 0.0545 - acc: 0.9833 - val_loss: 0.4383 - val_acc: 0.7667\n",
      "Epoch 134/150\n",
      "120/120 [==============================] - 0s - loss: 0.0570 - acc: 0.9833 - val_loss: 0.2740 - val_acc: 0.8667\n",
      "Epoch 135/150\n",
      "120/120 [==============================] - 0s - loss: 0.0516 - acc: 0.9917 - val_loss: 0.3732 - val_acc: 0.8000\n",
      "Epoch 136/150\n",
      "120/120 [==============================] - 0s - loss: 0.0518 - acc: 0.9833 - val_loss: 0.4047 - val_acc: 0.7667\n",
      "Epoch 137/150\n",
      "120/120 [==============================] - 0s - loss: 0.0500 - acc: 0.9833 - val_loss: 0.3423 - val_acc: 0.8000\n",
      "Epoch 138/150\n",
      "120/120 [==============================] - 0s - loss: 0.0501 - acc: 0.9833 - val_loss: 0.2891 - val_acc: 0.8333\n",
      "Epoch 139/150\n",
      "120/120 [==============================] - 0s - loss: 0.0510 - acc: 0.9917 - val_loss: 0.3247 - val_acc: 0.8000\n",
      "Epoch 140/150\n",
      "120/120 [==============================] - 0s - loss: 0.0500 - acc: 0.9833 - val_loss: 0.3235 - val_acc: 0.8000\n",
      "Epoch 141/150\n",
      "120/120 [==============================] - 0s - loss: 0.0493 - acc: 0.9833 - val_loss: 0.3897 - val_acc: 0.8000\n",
      "Epoch 142/150\n",
      "120/120 [==============================] - 0s - loss: 0.0501 - acc: 0.9833 - val_loss: 0.3146 - val_acc: 0.8000\n",
      "Epoch 143/150\n",
      "120/120 [==============================] - 0s - loss: 0.0491 - acc: 0.9833 - val_loss: 0.3446 - val_acc: 0.8000\n",
      "Epoch 144/150\n",
      "120/120 [==============================] - 0s - loss: 0.0510 - acc: 0.9833 - val_loss: 0.3746 - val_acc: 0.8000\n",
      "Epoch 145/150\n",
      "120/120 [==============================] - 0s - loss: 0.0491 - acc: 0.9833 - val_loss: 0.2467 - val_acc: 0.8667\n",
      "Epoch 146/150\n",
      "120/120 [==============================] - 0s - loss: 0.0481 - acc: 0.9833 - val_loss: 0.3461 - val_acc: 0.8000\n",
      "Epoch 147/150\n",
      "120/120 [==============================] - 0s - loss: 0.0468 - acc: 0.9833 - val_loss: 0.3750 - val_acc: 0.8000\n",
      "Epoch 148/150\n",
      "120/120 [==============================] - 0s - loss: 0.0470 - acc: 0.9917 - val_loss: 0.2987 - val_acc: 0.8000\n",
      "Epoch 149/150\n",
      "120/120 [==============================] - 0s - loss: 0.0467 - acc: 0.9833 - val_loss: 0.3665 - val_acc: 0.8000\n",
      "Epoch 150/150\n",
      "120/120 [==============================] - 0s - loss: 0.0470 - acc: 0.9833 - val_loss: 0.3071 - val_acc: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8f1cd2c208>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X, dummy_y, validation_split=0.2, nb_epoch=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluate Model\n",
    "\n",
    "We have trained our neural network on the entire dataset and we can evaluate the performance of the network by using the evaluation() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/150 [=====>........................] - ETA: 0sacc: 95.33%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, dummy_y)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
